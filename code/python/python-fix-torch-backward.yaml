id: python-fix-torch-backward

info:
  name: Python Fix Torch Backward
  author: carlini, phibenz
  difficulty: hard
  description: Fix a Python program that uses PyTorch's backward function.
  metadata:
    reference: https://github.com/carlini/yet-another-applied-llm-benchmark
  tags:
    - code
    - python
    - fix
    - torch
    - backward

setup:
  - type: write-to-file
    file-path: /tmp/pragmatics/python-fix-tokenizer/test_tokenizer.py
    code: |-
      from tokenizer import Tokenizer
      tokens = Tokenizer('sin(3+2*4)-cos(15)').tokens
      print(tokens)
      result = ['sin', '(', '3', '+', '2', '*', '4', ')', '-', 'cos', '(', '15', ')']
      if tokens == result:
          print("Tokenizer")
      else:
          print("Wrong")

query:
  - name: question
    text: |-
      Fix the bug in this code. Rewrite the entire code exactly as-is but just fix any bugs.

      ```python
      import torch
      import torch.nn as nn
      import torch.optim as optim

      class SimpleNet(nn.Module):
          def __init__(self):
              super(SimpleNet, self).__init__()
              self.fc = nn.Linear(1, 1)

          def forward(self, x):
              return self.fc(x)

      def PGD(model, input, target, loss_fn, epsilon, alpha, num_iter):
          perturbation = torch.zeros_like(input, requires_grad=True)

          for i in range(num_iter):
              output = model(input + perturbation)
              loss = loss_fn(output, target)

              loss.backward()

              # Update perturbation
              perturbation_grad = perturbation.grad.data
              perturbation.data = perturbation.data + alpha * perturbation_grad.sign()
              perturbation.data = torch.clamp(perturbation.data, -epsilon, epsilon)
              perturbation.grad.data.zero_()

          return input + perturbation

      model = SimpleNet()

      input = torch.tensor([[1.0]], requires_grad=True)
      target = torch.tensor([[2.0]])

      loss_fn = nn.MSELoss()

      epsilon = 0.1
      alpha = 0.01
      num_iter = 2

      adversarial_input = PGD(model, input, target, loss_fn, epsilon, alpha, num_iter)
      print("Resulting advex", adversarial_input)
      ```

    processors:
      - type: extract-code
      - type: write-to-file
        file-path: /tmp/pragmatics/python-fix-torch-backward/test.py
        file-type: python
      - type: run-file
        file-path: /tmp/pragmatics/python-fix-torch-backward/test.py
        file-type: python

    evaluators:
      - type: word
        words:
          - "Resulting advex tensor([["
          - "grad_fn=<Add"
          